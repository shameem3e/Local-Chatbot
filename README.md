# ğŸ“š Local Chatbot  

A **private ChatGPT-like application** that integrates multiple model providers â€” **Ollama (local)**, **Groq (cloud API)**, and **OpenAI (cloud API)** â€” into one unified chatbot interface. This allows you to experiment with both **local lightweight models** and **state-of-the-art hosted models** seamlessly.  

---

## ğŸ“Œ Features
- ğŸ–¥ï¸ **Run local models** (Mistral, Gemma via Ollama) without internet.  
- â˜ï¸ **Access hosted models** (Groq & OpenAI) with blazing speed.  
- ğŸ”‘ **Switch providers easily** using `models_config.yaml`.  
- âš¡ Lightweight backend + frontend for chat interactions.  
- ğŸ”’ Private setup â€” no external tracking.  

---

## ğŸ“‚ Folder Structure
Local-Chatbot/

â”œâ”€â”€ backend/ # Backend code (FastAPI or similar)

â”‚ â”œâ”€â”€ api/ # API endpoints and related services

â”‚ â”‚ â”œâ”€â”€ chat.py # Main chat API logic

â”‚ â”‚ â”œâ”€â”€ conversation.py # Handles conversation/session logic

â”‚ â”‚ â”œâ”€â”€ db_services.py # Database service functions

â”‚ â”‚ â”œâ”€â”€ provider_models.py # Model provider integration logic

â”‚ â”‚ â””â”€â”€ title.py # Utility for chat titles

â”‚ â”‚

â”‚ â”œâ”€â”€ config/ # Backend configuration

â”‚ â”‚ â”œâ”€â”€ init.py

â”‚ â”‚ â”œâ”€â”€ backend_config.py # Backend settings

â”‚ â”‚ â””â”€â”€ models_config.yaml # Defines available models & providers

â”‚ â”‚

â”‚ â”œâ”€â”€ db/ # Database access objects

â”‚ â”‚ â”œâ”€â”€ init.py

â”‚ â”‚ â””â”€â”€ chat_da o.py # Chat database access operations

â”‚ â”‚

â”‚ â”œâ”€â”€ llm/ # LLM factory/abstractions

â”‚ â”‚ â”œâ”€â”€ init.py

â”‚ â”‚ â””â”€â”€ llm_factory.py # Factory pattern to select models

â”‚ â”‚

â”‚ â”œâ”€â”€ services/ # Backend service layer

â”‚ â”‚ â”œâ”€â”€ chat.py

â”‚ â”‚ â”œâ”€â”€ db_services.py

â”‚ â”‚ â”œâ”€â”€ provider_models.py

â”‚ â”‚ â””â”€â”€ title.py

â”‚ â”‚

â”‚ â””â”€â”€ main.py # Backend entry point

â”‚

â”œâ”€â”€ data/ # Data directory

â”‚ â””â”€â”€ placeholder.txt

â”‚

â”œâ”€â”€ frontend/ # Frontend app (Streamlit)

â”‚ â”œâ”€â”€ config/

â”‚ â”‚ â”œâ”€â”€ init.py

â”‚ â”‚ â””â”€â”€ frontend_config.py # Frontend configuration

â”‚ â”œâ”€â”€ init.py

â”‚ â””â”€â”€ app.py # Streamlit app entry point

â”‚

â”œâ”€â”€ .env # Environment variables (API keys, configs)

â””â”€â”€ requirements.txt # Python dependencies

---

## â€‹â€‹ Setup Guide

### **1ï¸âƒ£ Install Prerequisites**
Before you begin, make sure you have:

- **Python 3.8+** installed on your system
- (If using local models) **Ollama** installed and configured
- API keys for any hosted models (if applicable)

#### Local Models (Ollama)
- Install via [Ollama Download](https://ollama.com/download/) for Windows.
- Run in PowerShell:
```powershell
ollama run mistral        # installs and runs Mistral:7B by default
ollama run gemma:2b       # installs and runs Gemma 2B
```
#### Hosted Models (API-based)

If you integrate hosted LLMs (e.g., Groq or OpenAI):

* Groq: Register at [Groq Cloud Console](https://console.groq.com/home), get your API key, then:
- Run in Terminal:
```terminal
pip install groq
setx GROQ_API_KEY "your_api_key_here"
```
* OpenAI: Sign up on [OpenAI platform](https://platform.openai.com), get API key, then:
- Run in Terminal:
```terminal
pip install openai
setx OPENAI_API_KEY "your_api_key_here"
```
### **2ï¸âƒ£ Clone the Repository**
```bash
git clone https://github.com/shameem3e/Local-Chatbot.git
cd Local-Chatbot
```

### **3ï¸âƒ£ Create Virtual Environment**
```bash
python -m venv venv
source venv/bin/activate   # Linux/Mac
venv\Scripts\activate      # Windows
```
### **4ï¸âƒ£ Install dependencies**
```bash
pip install --upgrade pip
pip install -r requirements.txt
```

### **5ï¸âƒ£ Run the project**
Launch both backend and frontend:
* Backend (e.g., FastAPI):
- Run in Terminal:
```terminal
uvicorn backend.main:app --reload
```
This will show to open `localhost:8501` 

* Frontend (Streamlit UI):
- Run in new Terminal(Don't close Backend terminal):
```terminal
streamlit run frontend/app.py
```
This will shows to open `localhost:8000`, Which allows you to open Chatbot in your browser.

### **ğŸ’» Command Reference**
```bash
Open New Chat
Select your model upon clicking settings icon
Enter your Text
```
### **âœ… Example Output**
You:
```bash
Hello, what can you do?
```
Local Chatbot:
```bash
Hi! I can chat using local LLMs (via Ollama) or call Groq/OpenAI models based on your config.
```

---

## ğŸ“œ Code Overview

### **Backend** 
 `backend/main.py` 
* **Purpose**: Entry point for running the backend (FastAPI server). 
* **Libraries**: 
	* `fastapi` â†’ to define the API server. 
	* `uvicorn` â†’ ASGI server to run FastAPI. 
* **Why**: Exposes backend services for chatbot through API.

---

#### **API Layer**(`backend/api/`) 
 `backend/api/chat.py` 
* **Purpose**: Handles chat requests and responses via API endpoints.
* **Libraries**: 
	* `fastapi.APIRouter` â†’ for route definition. 
	* `pydantic` â†’ request/response validation.
* **Why**: Main communication bridge between frontend and chatbot backend.

 `backend/api/conversation.py` 
* **Purpose**: Manages conversation state and sessions.
* **Libraries**: 
	* Pythonâ€™s `uuid` or session libraries â†’ for unique conversation IDs.
	* **Why**: Keeps track of ongoing multi-turn dialogues.

 `backend/api/db_services.py` 
* **Purpose**: API wrapper to fetch and store chat data in database.
* **Libraries**: 
	* `sqlalchemy` or `sqlite3` â†’ database operations.
* **Why**: Decouples DB logic from core APIs.

 `backend/api/provider_models.py` 
* **Purpose**: Defines which external LLM provider (Ollama, Groq, OpenAI) to call.
* **Libraries**: 
	* `httpx` / `requests` â†’ HTTP calls to providers.
* **Why**: Makes chatbot extensible across multiple model providers.

 `backend/api/title.py` 
* **Purpose**: Generates titles for chat sessions automatically.
* **Libraries**: 
	* LLM API calls (OpenAI, etc.)
* **Why**: Improves user experience by organizing conversations.

---

#### **Config Layer**(`backend/config/`) 
 `backend/config/backend_config.py` 
* **Purpose**: Stores backend-specific configuration (server host, ports).
* **Libraries**: 
	* `pydantic.BaseSettings` / `os` â†’ environment configs.
* **Why**: Keeps environment-dependent variables in one place.

 `backend/config/models_config.yaml` 
* **Purpose**: YAML file defining which LLM models/providers are available.
* **Libraries**: 
	* `pyyaml` â†’ parse YAML.
* **Why**: Easy to add/remove LLM models without changing code.

---

#### **Database Layer**(`backend/db/`) 
 `backend/db/chat_dao.py` 
* **Purpose**: DAO (Data Access Object) for chat records. Encapsulates database queries.
* **Libraries**: 
	* `sqlalchemy` or `sqlite3` 
* **Why**: Ensures clean DB operations and separation from business logic.

---

#### **LLM Layer**(`backend/llm/`) 
 `backend/llm/llm_factory.py` 
* **Purpose**: Factory pattern to dynamically load the correct model provider (Ollama, Groq, OpenAI).
* **Libraries**: 
	* Python OOP (classes, factory pattern). 
* **Why**: Makes chatbot provider-agnostic and modular.

---

#### **Service Layer**(`backend/service/`)
These mirror `api/` files but handle business logic (decoupled from API definitions).
Â `backend/services/chat.py`
* **Purpose**: Core chat logic (generate response, clean input, call LLM).
* **Libraries**:
Â 	* LLM SDKs (Ollama, OpenAI, etc.).
* **Why**: Keeps backend logic separate from API endpoints.

`backend/services/db_services.py` 
* **Purpose**: Business layer for DB operations (save/retrieve chats).
* **Libraries**: 
	* `sqlalchemy` / `sqlite3` 
* **Why**: Avoids mixing raw DB code with API.

`backend/services/provider_models.py` 
* **Purpose**: Handles model API calls in service layer.
* **Libraries**: 
	* `requests` / `httpx` 
* **Why**: Abstracts away provider communication.

`backend/services/title.py` 
* **Purpose**: Business logic for title generation.
* **Libraries**: 
	* LLM API calls.
* **Why**: Title generation handled separately from API.

---

### **Frontend** 
 `frontend/app.py` 
* **Purpose**: Entry point for Streamlit frontend app. 
* **Libraries**: 
	* `streamlit` â†’ UI framework. 
	* `requests` â†’ calls backend API. 
* **Why**: Provides simple web interface for chatting.

---

#### **Frontend Config**(`frontend/config/`) 
 `frontend/config/frontend_config.py` 
* **Purpose**: Stores frontend-related configs (API endpoint URLs, theme).
* **Libraries**: 
	* `os`,`dotenv`
* **Why**: Keeps frontend settings centralized.

---

### **Data Folder** 
 `data/placeholder.txt` 
* **Purpose**: Keeps the data/ folder in Git (since empty folders arenâ€™t committed). 

---

### **Root Files** 
 `.env` 
* **Purpose**: Stores sensitive keys (OpenAI API, Groq API, DB connection strings). 
* **Libraries**: 
	* `python-dotenv`
* **Why**: Keeps secrets out of source code.

---

## â“ FAQ

**Q1: Do I need the internet to use this chatbot?**  
- No (for Ollama) â†’ You can run local LLMs like Mistral or Gemma entirely offline.
- Yes (for Groq / OpenAI) â†’ These providers require an active internet connection and API keys. 

**Q2: Can I use multiple models in one session?**  
- Yes! You can switch between providers (`Ollama`, `Groq`, `OpenAI`) seamlessly through `models_config.yaml` or frontend settings.

**Q3. Where are my conversations stored?**
- Chats are stored locally in the SQLite database (`backend/db/chat_dao.py`).
- Nothing is shared externally unless you explicitly use Groq/OpenAI APIs.

**Q4. How do I add a new model provider?**
- Update `models_config.yaml` with the new provider details.
- Extend `llm_factory.py` to load the provider.
- Add any required API key to `.env`.

**Q5. Can I run this on Windows/Mac/Linux?**
- Yes, itâ€™s fully cross-platform.
- For Windows users, ensure Ollama and Python are installed properly.

**Q6. Does this support GPU acceleration?**
- Ollama uses your local GPU automatically if available.
- Groq/OpenAI run on their own GPU-powered servers.

**Q7. Is my data private?**
- Yes. All local chats stay on your system.
- Only requests sent to Groq/OpenAI APIs leave your machine.  


**Q3: Can I mark tasks as done or delete them?**  
- Yes, using `done <id>` or `delete <id>` commands.  

**Q4: How do recurring tasks work?**  
- You can specify recurrence like `"daily 07:00"`, `"every monday 09:30"`, or `"monthly day 1 10:00"`.  

**Q5: How can I export tasks to a calendar?**  
- Use the `export` command to generate `.ics` files.  

**Q6: Can I run it on Windows/macOS/Linux?**  
- Yes, itâ€™s cross-platform. Make sure you have Python 3.10+ and the required toolchain for `llama-cpp-python`.

---

## ğŸ›  Tech Stack

### Backend
- **FastAPI** â€“ Async API framework for performance.  
- **Uvicorn** â€“ Runs the FastAPI backend.  
- **SQLAlchemy** / **sqlite3** â€“ Lightweight DB storage. 
- **PyYAML** â€“ Parse `models_config.yaml` for providers.  
- **httpx** â€“ `Async HTTP client for API calls.
- **python-dotenv** â€“ Manage environment variables.  

---
### LLM Providers
- **Ollama** â€“ Local models like Mistral, Gemma.  
- **Groq API** â€“ High-speed cloud inference.  
- **OpenAI API** â€“ Access GPT family of models.

---
### Frontend
- **Streamlit** â€“ Simple, interactive UI.  
- **Requests** â€“ Connect frontend to backend. 

---
### General
- **Python 3.8+** â€“ Base language.  
- **dotenv / os** â€“ Config handling.  
- **logging** â€“ Debugging & monitoring.

---

## ğŸš€ Future Improvements

- ğŸ” **RAG Integration** â†’ Enhance answers with local document knowledge.
- ğŸ‘¥ **Multi-user Support** â†’ Separate sessions for multiple users.
- ğŸ’¾ **Export/Import Chats** â†’ Save or restore chat history easily.
- ğŸ¨ **Enhanced UI** â†’ More customization in frontend (themes, markdown, rich media).
- ğŸ“Š **Analytics Dashboard** â†’ Track usage, model performance, and latency.
- ğŸ§  **Custom Fine-tuned Models** â†’ Support user-uploaded fine-tuned LLMs.
- ğŸ“¡ **WebSocket Support** â†’ Real-time streaming responses (like ChatGPT).

---

## ğŸ‘¨â€ğŸ’» Author
[MD. Shameem Ahammed](https://sites.google.com/view/shameem3e)

Graduate Student | AI & ML Enthusiast

---
